{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d0a92b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f72be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LlamaConfig:\n",
    "    vocab_size: int  # (토큰 id 갯수)\n",
    "    hidden_size: int # (토큰당 임베딩 차원 크기)\n",
    "    intermediate_size: int # (MLP 막 거친 차원 크기)\n",
    "    num_hidden_layers: int # (디코딩 레이어 갯수)\n",
    "    attention_heads: int # Query 헤드\n",
    "    key_value_heads: int # KV 헤드\n",
    "    hidden_act: str # (FFN 활성화 함수)\n",
    "    max_position_embeddings: int #(시퀀스 길이)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8bdaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        rms_x = x.square().mean(dim=-1, keepdim=True).sqrt()\n",
    "        return self.gamma * x / (rms_x + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99162d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(RoPE, self).__init__()\n",
    "        d_indices = torch.arange(0, hidden_size / 2, dtype=torch.float32)  # size: hidden_size / 2\n",
    "        theta = torch.exp(d_indices * -2 * math.log(10000) / hidden_size)  # 10000^(-2i/d) == exp((-2i/d) * log(10000))\n",
    "        \n",
    "        self.register_buffer('theta', theta) # device 연동\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):  # x: (n, seq_len, hidden_size)\n",
    "        seq_len = x.size(-2)\n",
    "        m_theta = torch.matmul(torch.arange(seq_len).float().view(-1, 1), self.theta.view(1, -1))\n",
    "        m_theta = m_theta.repeat_interleave(2, dim=-1) # (1, 2, 3) -> (1, 1, 2, 2, 3, 3)\n",
    "\n",
    "        x_flip = torch.stack([-x[:, :, 1::2], x[:, :, ::2], ], dim=-1).flatten(start_dim=-2)  # -x2, x1, -x4, x3 ...\n",
    "        rotated_x = torch.mul(x, torch.cos(m_theta)) + torch.mul(x_flip, torch.sin(m_theta))\n",
    "        return rotated_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b16246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPESelfAttention(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super(RoPESelfAttention, self).__init__()\n",
    "\n",
    "        self.num_attention_heads = config.attention_heads\n",
    "        self.head_size = int(config.hidden_size / self.num_attention_heads)\n",
    "        \n",
    "        self.query = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
    "        self.key = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
    "        self.value = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "        self.rms_norm = RMSNorm(config.hidden_size) # 2개?\n",
    "        self.rope = RoPE(config.hidden_size)\n",
    "    \n",
    "    def to_multi_heads(self, x: torch.Tensor):\n",
    "        n, seq_len, _ = x.size()\n",
    "        mh = x.view(n, seq_len, self.num_attention_heads, self.head_size)\n",
    "        return mh.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):  # x: (n, seq_len, hidden_size)\n",
    "        x_norm = self.rms_norm(x)\n",
    "        q = self.rope(self.query(x_norm))\n",
    "        k = self.rope(self.key(x_norm))\n",
    "        v = self.value(x_norm)\n",
    "        \n",
    "        q = self.to_multi_heads(q) # q, k, v (n, num_heads, seq_len, hidden_size)\n",
    "        k = self.to_multi_heads(k)\n",
    "        v = self.to_multi_heads(v)\n",
    "\n",
    "        atten_score = torch.matmul(q, k.transpose(-1, -2))\n",
    "        \n",
    "        # head_size scaling\n",
    "        \n",
    "        # add attention mask\n",
    "        \n",
    "        # get probs\n",
    "        attention_probs = torch.softmax(atten_score, dim=-1)\n",
    "        \n",
    "        # dropout\n",
    "        \n",
    "        context = torch.matmul(attention_probs, v)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous().view(x.size()) # why contiguous?\n",
    "        return x + context\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5e0bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNSwiGLU(nn.Module):\n",
    "    def __init__(self, hidden_size: int, intermediate_size: int):\n",
    "        super(FFNSwiGLU, self).__init__()\n",
    "        hidden_dim = int(2 * intermediate_size / 3) # 3072 -> 2048\n",
    "        \n",
    "        self.w1 = nn.Linear(hidden_size, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(hidden_size, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, hidden_size, bias=False)\n",
    "\n",
    "    def _swish(self, x: torch.Tensor):\n",
    "        return x * x.sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        swish = F.silu(self.w1(x))  # swish = self._swish(self.w1(x))\n",
    "        swish_v = swish * self.v(x)\n",
    "        return self.w2(swish_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "378a1a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama2Decoder(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super(Llama2Decoder, self).__init__()\n",
    "        self.attn = RoPESelfAttention(config)\n",
    "        self.rms_norm = RMSNorm(config.hidden_size)\n",
    "        self.ffn = FFNSwiGLU(config.hidden_size, config.intermediate_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        context = self.attn(x)\n",
    "        context_norm = self.rms_norm(context)\n",
    "\n",
    "        # ffnswiglu\n",
    "        output = self.ffn(context_norm)\n",
    "        return context + output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfd2720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama2Model(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super(Llama2Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.decoders = nn.ModuleList([Llama2Decoder(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.rms_norm = RMSNorm(config.hidden_size)\n",
    "\n",
    "        self.ffn = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.embedding(x)\n",
    "        # return x \n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x)\n",
    "        \n",
    "        x = self.rms_norm(x)\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5959deb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.Size([32, 20])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 15000\n",
    "hidden_size = 256\n",
    "batch_size = 32\n",
    "seq_len = 20\n",
    "\n",
    "x = torch.randint(1, 15000, size=(batch_size, seq_len))\n",
    "print(x.dtype)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a6c3a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Llama2Model(\n",
       "  (embedding): Embedding(15000, 256)\n",
       "  (decoders): ModuleList(\n",
       "    (0-1): 2 x Llama2Decoder(\n",
       "      (attn): RoPESelfAttention(\n",
       "        (query): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (rms_norm): RMSNorm()\n",
       "        (rope): RoPE()\n",
       "      )\n",
       "      (rms_norm): RMSNorm()\n",
       "      (ffn): FFNSwiGLU(\n",
       "        (w1): Linear(in_features=256, out_features=2048, bias=False)\n",
       "        (v): Linear(in_features=256, out_features=2048, bias=False)\n",
       "        (w2): Linear(in_features=2048, out_features=256, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (rms_norm): RMSNorm()\n",
       "  (ffn): Linear(in_features=256, out_features=15000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = LlamaConfig(\n",
    "    vocab_size=15000, \n",
    "    hidden_size=hidden_size, \n",
    "    intermediate_size=3072, \n",
    "    num_hidden_layers=2, \n",
    "    attention_heads=8, \n",
    "    key_value_heads=1, \n",
    "    hidden_act=\"\", \n",
    "    max_position_embeddings=1\n",
    ")\n",
    "\n",
    "llama_model = Llama2Model(config)\n",
    "llama_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99cfd686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 15000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = llama_model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2bef86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
