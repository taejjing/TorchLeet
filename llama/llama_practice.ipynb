{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d0a92b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f72be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LlamaConfig:\n",
    "    vocab_size: int  # (토큰 id 갯수)\n",
    "    hidden_size: int # (토큰당 임베딩 차원 크기)\n",
    "    intermediate_size: int # (MLP 막 거친 차원 크기)\n",
    "    num_hidden_layers: int # (디코딩 레이어 갯수)\n",
    "    attention_heads: int # Query 헤드\n",
    "    key_value_heads: int # KV 헤드\n",
    "    hidden_act: str # (FFN 활성화 함수)\n",
    "    max_position_embeddings: int #(시퀀스 길이)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8bdaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        rms_x = x.square().mean(dim=-1, keepdim=True).sqrt()\n",
    "        return self.gamma * x / (rms_x + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99162d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(RoPE, self).__init__()\n",
    "        d_indices = torch.arange(0, hidden_size / 2, dtype=torch.float32)  # size: hidden_size / 2\n",
    "        theta = torch.exp(d_indices * -2 * math.log(10000) / hidden_size)  # 10000^(-2i/d) == exp((-2i/d) * log(10000))\n",
    "        \n",
    "        self.register_buffer('theta', theta) # device 연동\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):  # x: (n, seq_len, hidden_size)\n",
    "        seq_len = x.size(-2)\n",
    "        m_theta = torch.matmul(torch.arange(seq_len).float().view(-1, 1), self.theta.view(1, -1))\n",
    "        m_theta = m_theta.repeat_interleave(2, dim=-1) # (1, 2, 3) -> (1, 1, 2, 2, 3, 3)\n",
    "\n",
    "        x_flip = torch.stack([-x[:, :, 1::2], x[:, :, ::2], ], dim=-1).flatten(start_dim=-2)  # -x2, x1, -x4, x3 ...\n",
    "        rotated_x = torch.mul(x, torch.cos(m_theta)) + torch.mul(x_flip, torch.sin(m_theta))\n",
    "        return rotated_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "878bf1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 256])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "hidden_size = 256\n",
    "seq_len = 20\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233c6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "rope = RoPE(hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d9e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 256])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rope(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30644589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 256])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_theta = torch.matmul(torch.arange(seq_len).float().view(-1, 1), rope.theta.view(1, -1))\n",
    "m_theta = m_theta.repeat_interleave(2, dim=-1)\n",
    "m_theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "23163da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [1.0000e+00, 1.0000e+00, 9.3057e-01,  ..., 1.1548e-04, 1.0746e-04,\n",
       "         1.0746e-04],\n",
       "        [2.0000e+00, 2.0000e+00, 1.8611e+00,  ..., 2.3096e-04, 2.1492e-04,\n",
       "         2.1492e-04],\n",
       "        ...,\n",
       "        [1.7000e+01, 1.7000e+01, 1.5820e+01,  ..., 1.9631e-03, 1.8268e-03,\n",
       "         1.8268e-03],\n",
       "        [1.8000e+01, 1.8000e+01, 1.6750e+01,  ..., 2.0786e-03, 1.9343e-03,\n",
       "         1.9343e-03],\n",
       "        [1.9000e+01, 1.9000e+01, 1.7681e+01,  ..., 2.1941e-03, 2.0418e-03,\n",
       "         2.0418e-03]])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c21f76c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 256])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(x, m_theta).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d57a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3., 3.],\n",
       "        [4., 4., 4., 4., 4.],\n",
       "        [5., 5., 5., 5., 5.]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.arange(1, 6).float().view(5, 1), torch.ones(5).view(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "27864e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.stack([torch.stack([torch.arange(1, hidden_size + 1)] * seq_len)] * batch_size)\n",
    "dummy_flip = torch.stack([-dummy[:, :, 1::2], dummy[:, :, ::2], ], dim=-1).flatten(start_dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "22de402a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         ...,\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256]],\n",
       "\n",
       "        [[  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         ...,\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256]],\n",
       "\n",
       "        [[  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         ...,\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         ...,\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256]],\n",
       "\n",
       "        [[  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         ...,\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256]],\n",
       "\n",
       "        [[  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         ...,\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256],\n",
       "         [  1,   2,   3,  ..., 254, 255, 256]]])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "20bd047b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.0000e+00, 2.0000e+00, 2.7917e+00,  ..., 2.9331e-02,\n",
       "          2.7402e-02, 2.7510e-02],\n",
       "         [2.0000e+00, 4.0000e+00, 5.5834e+00,  ..., 5.8663e-02,\n",
       "          5.4805e-02, 5.5020e-02],\n",
       "         ...,\n",
       "         [1.7000e+01, 3.4000e+01, 4.7459e+01,  ..., 4.9863e-01,\n",
       "          4.6584e-01, 4.6767e-01],\n",
       "         [1.8000e+01, 3.6000e+01, 5.0251e+01,  ..., 5.2797e-01,\n",
       "          4.9324e-01, 4.9518e-01],\n",
       "         [1.9000e+01, 3.8000e+01, 5.3043e+01,  ..., 5.5730e-01,\n",
       "          5.2065e-01, 5.2269e-01]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.0000e+00, 2.0000e+00, 2.7917e+00,  ..., 2.9331e-02,\n",
       "          2.7402e-02, 2.7510e-02],\n",
       "         [2.0000e+00, 4.0000e+00, 5.5834e+00,  ..., 5.8663e-02,\n",
       "          5.4805e-02, 5.5020e-02],\n",
       "         ...,\n",
       "         [1.7000e+01, 3.4000e+01, 4.7459e+01,  ..., 4.9863e-01,\n",
       "          4.6584e-01, 4.6767e-01],\n",
       "         [1.8000e+01, 3.6000e+01, 5.0251e+01,  ..., 5.2797e-01,\n",
       "          4.9324e-01, 4.9518e-01],\n",
       "         [1.9000e+01, 3.8000e+01, 5.3043e+01,  ..., 5.5730e-01,\n",
       "          5.2065e-01, 5.2269e-01]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.0000e+00, 2.0000e+00, 2.7917e+00,  ..., 2.9331e-02,\n",
       "          2.7402e-02, 2.7510e-02],\n",
       "         [2.0000e+00, 4.0000e+00, 5.5834e+00,  ..., 5.8663e-02,\n",
       "          5.4805e-02, 5.5020e-02],\n",
       "         ...,\n",
       "         [1.7000e+01, 3.4000e+01, 4.7459e+01,  ..., 4.9863e-01,\n",
       "          4.6584e-01, 4.6767e-01],\n",
       "         [1.8000e+01, 3.6000e+01, 5.0251e+01,  ..., 5.2797e-01,\n",
       "          4.9324e-01, 4.9518e-01],\n",
       "         [1.9000e+01, 3.8000e+01, 5.3043e+01,  ..., 5.5730e-01,\n",
       "          5.2065e-01, 5.2269e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.0000e+00, 2.0000e+00, 2.7917e+00,  ..., 2.9331e-02,\n",
       "          2.7402e-02, 2.7510e-02],\n",
       "         [2.0000e+00, 4.0000e+00, 5.5834e+00,  ..., 5.8663e-02,\n",
       "          5.4805e-02, 5.5020e-02],\n",
       "         ...,\n",
       "         [1.7000e+01, 3.4000e+01, 4.7459e+01,  ..., 4.9863e-01,\n",
       "          4.6584e-01, 4.6767e-01],\n",
       "         [1.8000e+01, 3.6000e+01, 5.0251e+01,  ..., 5.2797e-01,\n",
       "          4.9324e-01, 4.9518e-01],\n",
       "         [1.9000e+01, 3.8000e+01, 5.3043e+01,  ..., 5.5730e-01,\n",
       "          5.2065e-01, 5.2269e-01]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.0000e+00, 2.0000e+00, 2.7917e+00,  ..., 2.9331e-02,\n",
       "          2.7402e-02, 2.7510e-02],\n",
       "         [2.0000e+00, 4.0000e+00, 5.5834e+00,  ..., 5.8663e-02,\n",
       "          5.4805e-02, 5.5020e-02],\n",
       "         ...,\n",
       "         [1.7000e+01, 3.4000e+01, 4.7459e+01,  ..., 4.9863e-01,\n",
       "          4.6584e-01, 4.6767e-01],\n",
       "         [1.8000e+01, 3.6000e+01, 5.0251e+01,  ..., 5.2797e-01,\n",
       "          4.9324e-01, 4.9518e-01],\n",
       "         [1.9000e+01, 3.8000e+01, 5.3043e+01,  ..., 5.5730e-01,\n",
       "          5.2065e-01, 5.2269e-01]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00],\n",
       "         [1.0000e+00, 2.0000e+00, 2.7917e+00,  ..., 2.9331e-02,\n",
       "          2.7402e-02, 2.7510e-02],\n",
       "         [2.0000e+00, 4.0000e+00, 5.5834e+00,  ..., 5.8663e-02,\n",
       "          5.4805e-02, 5.5020e-02],\n",
       "         ...,\n",
       "         [1.7000e+01, 3.4000e+01, 4.7459e+01,  ..., 4.9863e-01,\n",
       "          4.6584e-01, 4.6767e-01],\n",
       "         [1.8000e+01, 3.6000e+01, 5.0251e+01,  ..., 5.2797e-01,\n",
       "          4.9324e-01, 4.9518e-01],\n",
       "         [1.9000e+01, 3.8000e+01, 5.3043e+01,  ..., 5.5730e-01,\n",
       "          5.2065e-01, 5.2269e-01]]])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(dummy, m_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e56cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "theta = torch.exp(torch.arange(0, hidden_size, 2, dtype=torch.float32) * -math.log(10000) / hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f278fda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 9.3057e-01, 8.6596e-01, 8.0584e-01, 7.4989e-01, 6.9783e-01,\n",
       "        6.4938e-01, 6.0430e-01, 5.6234e-01, 5.2330e-01, 4.8697e-01, 4.5316e-01,\n",
       "        4.2170e-01, 3.9242e-01, 3.6517e-01, 3.3982e-01, 3.1623e-01, 2.9427e-01,\n",
       "        2.7384e-01, 2.5483e-01, 2.3714e-01, 2.2067e-01, 2.0535e-01, 1.9110e-01,\n",
       "        1.7783e-01, 1.6548e-01, 1.5399e-01, 1.4330e-01, 1.3335e-01, 1.2409e-01,\n",
       "        1.1548e-01, 1.0746e-01, 1.0000e-01, 9.3057e-02, 8.6596e-02, 8.0584e-02,\n",
       "        7.4989e-02, 6.9783e-02, 6.4938e-02, 6.0430e-02, 5.6234e-02, 5.2330e-02,\n",
       "        4.8697e-02, 4.5316e-02, 4.2170e-02, 3.9242e-02, 3.6517e-02, 3.3982e-02,\n",
       "        3.1623e-02, 2.9427e-02, 2.7384e-02, 2.5483e-02, 2.3714e-02, 2.2067e-02,\n",
       "        2.0535e-02, 1.9110e-02, 1.7783e-02, 1.6548e-02, 1.5399e-02, 1.4330e-02,\n",
       "        1.3335e-02, 1.2409e-02, 1.1548e-02, 1.0746e-02, 1.0000e-02, 9.3057e-03,\n",
       "        8.6596e-03, 8.0584e-03, 7.4989e-03, 6.9783e-03, 6.4938e-03, 6.0430e-03,\n",
       "        5.6234e-03, 5.2330e-03, 4.8697e-03, 4.5316e-03, 4.2170e-03, 3.9242e-03,\n",
       "        3.6517e-03, 3.3982e-03, 3.1623e-03, 2.9427e-03, 2.7384e-03, 2.5483e-03,\n",
       "        2.3714e-03, 2.2067e-03, 2.0535e-03, 1.9110e-03, 1.7783e-03, 1.6548e-03,\n",
       "        1.5399e-03, 1.4330e-03, 1.3335e-03, 1.2409e-03, 1.1548e-03, 1.0746e-03,\n",
       "        1.0000e-03, 9.3057e-04, 8.6596e-04, 8.0584e-04, 7.4989e-04, 6.9783e-04,\n",
       "        6.4938e-04, 6.0430e-04, 5.6234e-04, 5.2330e-04, 4.8697e-04, 4.5316e-04,\n",
       "        4.2170e-04, 3.9242e-04, 3.6517e-04, 3.3982e-04, 3.1623e-04, 2.9427e-04,\n",
       "        2.7384e-04, 2.5483e-04, 2.3714e-04, 2.2067e-04, 2.0535e-04, 1.9110e-04,\n",
       "        1.7783e-04, 1.6548e-04, 1.5399e-04, 1.4330e-04, 1.3335e-04, 1.2409e-04,\n",
       "        1.1548e-04, 1.0746e-04])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ac34c51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.4147e-01, 8.4147e-01, 8.0196e-01, 8.0196e-01, 7.6172e-01, 7.6172e-01,\n",
       "        7.2141e-01, 7.2141e-01, 6.8156e-01, 6.8156e-01, 6.4256e-01, 6.4256e-01,\n",
       "        6.0469e-01, 6.0469e-01, 5.6818e-01, 5.6818e-01, 5.3317e-01, 5.3317e-01,\n",
       "        4.9974e-01, 4.9974e-01, 4.6795e-01, 4.6795e-01, 4.3781e-01, 4.3781e-01,\n",
       "        4.0931e-01, 4.0931e-01, 3.8242e-01, 3.8242e-01, 3.5711e-01, 3.5711e-01,\n",
       "        3.3332e-01, 3.3332e-01, 3.1098e-01, 3.1098e-01, 2.9004e-01, 2.9004e-01,\n",
       "        2.7043e-01, 2.7043e-01, 2.5208e-01, 2.5208e-01, 2.3492e-01, 2.3492e-01,\n",
       "        2.1889e-01, 2.1889e-01, 2.0391e-01, 2.0391e-01, 1.8993e-01, 1.8993e-01,\n",
       "        1.7689e-01, 1.7689e-01, 1.6473e-01, 1.6473e-01, 1.5338e-01, 1.5338e-01,\n",
       "        1.4281e-01, 1.4281e-01, 1.3296e-01, 1.3296e-01, 1.2378e-01, 1.2378e-01,\n",
       "        1.1522e-01, 1.1522e-01, 1.0725e-01, 1.0725e-01, 9.9833e-02, 9.9833e-02,\n",
       "        9.2923e-02, 9.2923e-02, 8.6488e-02, 8.6488e-02, 8.0497e-02, 8.0497e-02,\n",
       "        7.4919e-02, 7.4919e-02, 6.9726e-02, 6.9726e-02, 6.4893e-02, 6.4893e-02,\n",
       "        6.0393e-02, 6.0393e-02, 5.6204e-02, 5.6204e-02, 5.2306e-02, 5.2306e-02,\n",
       "        4.8678e-02, 4.8678e-02, 4.5300e-02, 4.5300e-02, 4.2157e-02, 4.2157e-02,\n",
       "        3.9232e-02, 3.9232e-02, 3.6509e-02, 3.6509e-02, 3.3976e-02, 3.3976e-02,\n",
       "        3.1618e-02, 3.1618e-02, 2.9423e-02, 2.9423e-02, 2.7381e-02, 2.7381e-02,\n",
       "        2.5480e-02, 2.5480e-02, 2.3712e-02, 2.3712e-02, 2.2066e-02, 2.2066e-02,\n",
       "        2.0534e-02, 2.0534e-02, 1.9108e-02, 1.9108e-02, 1.7782e-02, 1.7782e-02,\n",
       "        1.6547e-02, 1.6547e-02, 1.5399e-02, 1.5399e-02, 1.4330e-02, 1.4330e-02,\n",
       "        1.3335e-02, 1.3335e-02, 1.2409e-02, 1.2409e-02, 1.1548e-02, 1.1548e-02,\n",
       "        1.0746e-02, 1.0746e-02, 9.9998e-03, 9.9998e-03, 9.3056e-03, 9.3056e-03,\n",
       "        8.6595e-03, 8.6595e-03, 8.0583e-03, 8.0583e-03, 7.4989e-03, 7.4989e-03,\n",
       "        6.9782e-03, 6.9782e-03, 6.4938e-03, 6.4938e-03, 6.0429e-03, 6.0429e-03,\n",
       "        5.6234e-03, 5.6234e-03, 5.2330e-03, 5.2330e-03, 4.8697e-03, 4.8697e-03,\n",
       "        4.5316e-03, 4.5316e-03, 4.2170e-03, 4.2170e-03, 3.9242e-03, 3.9242e-03,\n",
       "        3.6517e-03, 3.6517e-03, 3.3982e-03, 3.3982e-03, 3.1623e-03, 3.1623e-03,\n",
       "        2.9427e-03, 2.9427e-03, 2.7384e-03, 2.7384e-03, 2.5483e-03, 2.5483e-03,\n",
       "        2.3714e-03, 2.3714e-03, 2.2067e-03, 2.2067e-03, 2.0535e-03, 2.0535e-03,\n",
       "        1.9110e-03, 1.9110e-03, 1.7783e-03, 1.7783e-03, 1.6548e-03, 1.6548e-03,\n",
       "        1.5399e-03, 1.5399e-03, 1.4330e-03, 1.4330e-03, 1.3335e-03, 1.3335e-03,\n",
       "        1.2409e-03, 1.2409e-03, 1.1548e-03, 1.1548e-03, 1.0746e-03, 1.0746e-03,\n",
       "        1.0000e-03, 1.0000e-03, 9.3057e-04, 9.3057e-04, 8.6596e-04, 8.6596e-04,\n",
       "        8.0584e-04, 8.0584e-04, 7.4989e-04, 7.4989e-04, 6.9783e-04, 6.9783e-04,\n",
       "        6.4938e-04, 6.4938e-04, 6.0430e-04, 6.0430e-04, 5.6234e-04, 5.6234e-04,\n",
       "        5.2330e-04, 5.2330e-04, 4.8697e-04, 4.8697e-04, 4.5316e-04, 4.5316e-04,\n",
       "        4.2170e-04, 4.2170e-04, 3.9242e-04, 3.9242e-04, 3.6517e-04, 3.6517e-04,\n",
       "        3.3982e-04, 3.3982e-04, 3.1623e-04, 3.1623e-04, 2.9427e-04, 2.9427e-04,\n",
       "        2.7384e-04, 2.7384e-04, 2.5483e-04, 2.5483e-04, 2.3714e-04, 2.3714e-04,\n",
       "        2.2067e-04, 2.2067e-04, 2.0535e-04, 2.0535e-04, 1.9110e-04, 1.9110e-04,\n",
       "        1.7783e-04, 1.7783e-04, 1.6548e-04, 1.6548e-04, 1.5399e-04, 1.5399e-04,\n",
       "        1.4330e-04, 1.4330e-04, 1.3335e-04, 1.3335e-04, 1.2409e-04, 1.2409e-04,\n",
       "        1.1548e-04, 1.1548e-04, 1.0746e-04, 1.0746e-04])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cos(theta.repeat_interleave(2))\n",
    "torch.sin(theta.repeat_interleave(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "24e7b15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 9.3057e-01, 8.6596e-01, 8.0584e-01, 7.4989e-01, 6.9783e-01,\n",
       "        6.4938e-01, 6.0430e-01, 5.6234e-01, 5.2330e-01, 4.8697e-01, 4.5316e-01,\n",
       "        4.2170e-01, 3.9242e-01, 3.6517e-01, 3.3982e-01, 3.1623e-01, 2.9427e-01,\n",
       "        2.7384e-01, 2.5483e-01, 2.3714e-01, 2.2067e-01, 2.0535e-01, 1.9110e-01,\n",
       "        1.7783e-01, 1.6548e-01, 1.5399e-01, 1.4330e-01, 1.3335e-01, 1.2409e-01,\n",
       "        1.1548e-01, 1.0746e-01, 1.0000e-01, 9.3057e-02, 8.6596e-02, 8.0584e-02,\n",
       "        7.4989e-02, 6.9783e-02, 6.4938e-02, 6.0430e-02, 5.6234e-02, 5.2330e-02,\n",
       "        4.8697e-02, 4.5316e-02, 4.2170e-02, 3.9242e-02, 3.6517e-02, 3.3982e-02,\n",
       "        3.1623e-02, 2.9427e-02, 2.7384e-02, 2.5483e-02, 2.3714e-02, 2.2067e-02,\n",
       "        2.0535e-02, 1.9110e-02, 1.7783e-02, 1.6548e-02, 1.5399e-02, 1.4330e-02,\n",
       "        1.3335e-02, 1.2409e-02, 1.1548e-02, 1.0746e-02, 1.0000e-02, 9.3057e-03,\n",
       "        8.6596e-03, 8.0584e-03, 7.4989e-03, 6.9783e-03, 6.4938e-03, 6.0430e-03,\n",
       "        5.6234e-03, 5.2330e-03, 4.8697e-03, 4.5316e-03, 4.2170e-03, 3.9242e-03,\n",
       "        3.6517e-03, 3.3982e-03, 3.1623e-03, 2.9427e-03, 2.7384e-03, 2.5483e-03,\n",
       "        2.3714e-03, 2.2067e-03, 2.0535e-03, 1.9110e-03, 1.7783e-03, 1.6548e-03,\n",
       "        1.5399e-03, 1.4330e-03, 1.3335e-03, 1.2409e-03, 1.1548e-03, 1.0746e-03,\n",
       "        1.0000e-03, 9.3057e-04, 8.6596e-04, 8.0584e-04, 7.4989e-04, 6.9783e-04,\n",
       "        6.4938e-04, 6.0430e-04, 5.6234e-04, 5.2330e-04, 4.8697e-04, 4.5316e-04,\n",
       "        4.2170e-04, 3.9242e-04, 3.6517e-04, 3.3982e-04, 3.1623e-04, 2.9427e-04,\n",
       "        2.7384e-04, 2.5483e-04, 2.3714e-04, 2.2067e-04, 2.0535e-04, 1.9110e-04,\n",
       "        1.7783e-04, 1.6548e-04, 1.5399e-04, 1.4330e-04, 1.3335e-04, 1.2409e-04,\n",
       "        1.1548e-04, 1.0746e-04])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.arange(0, hidden_size / 2, dtype=torch.float32) * -2 * math.log(10000) / hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b16246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPESelfAttention(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super(RoPESelfAttention, self).__init__()\n",
    "\n",
    "        self.num_attention_heads = config.attention_heads\n",
    "        self.head_size = int(config.hidden_size / self.num_attention_heads)\n",
    "        \n",
    "        self.query = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
    "        self.key = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
    "        self.value = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "        self.rms_norm = RMSNorm(config.hidden_size) # 2개?\n",
    "        self.rope = RoPE(config.hidden_size)\n",
    "    \n",
    "    def to_multi_heads(self, x: torch.Tensor):\n",
    "        n, seq_len, _ = x.size()\n",
    "        mh = x.view(n, seq_len, self.num_attention_heads, self.head_size)\n",
    "        return mh.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):  # x: (n, seq_len, hidden_size)\n",
    "        x_norm = self.rms_norm(x)\n",
    "        q = self.rope(self.query(x_norm))\n",
    "        k = self.rope(self.key(x_norm))\n",
    "\n",
    "        v = self.value(x_norm)\n",
    "\n",
    "        q = self.to_multi_heads(q) # q: (n, num_heads, seq_len, hidden_size)\n",
    "        k = self.to_multi_heads(k)\n",
    "        v = self.to_multi_heads(v)\n",
    "\n",
    "        atten_score = torch.matmul(q, k.transpose(-1, -2))\n",
    "        \n",
    "        # head_size scaling\n",
    "        \n",
    "        # add attention mask\n",
    "        \n",
    "        # get probs\n",
    "        attention_probs = torch.softmax(atten_score, dim=-1)\n",
    "        \n",
    "        # dropout\n",
    "\n",
    "        # matmul value\n",
    "        context = torch.matmul(attention_probs, v)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous().view(x.size()) # why contiguous?\n",
    "        return x + context\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5959deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LlamaConfig(1, hidden_size, 1, 1, 8, 1, \"\", 1)\n",
    "attn = RoPESelfAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5b49d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d26d3285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 256])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6a6852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55b01048",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = nn.Linear(hidden_size, hidden_size)\n",
    "k = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "q = q(x)\n",
    "k = k(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2de50d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 20, 32])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_mh = attn.to_multi_heads(q)\n",
    "k_mh = attn.to_multi_heads(k)\n",
    "q_mh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf3077a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 32, 20])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_mh.transpose(-1, -2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd1705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 20, 20])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_score = torch.matmul(q_mh, k_mh.transpose(-1, -2)) \n",
    "attn_score.shape # (n, num_heads, seq_len, seq_len) # 단어들의 attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cfd686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooler(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.rms_norm = RMSNorm(config.hidden_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_norm = self.rms_norm(x)\n",
    "\n",
    "        # silu\n",
    "\n",
    "        # ff\n",
    "\n",
    "        # add x\n",
    "\n",
    "        # return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a23bbaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama2Decoder(nn.Module):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama2Model(nn.Module):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e7169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
